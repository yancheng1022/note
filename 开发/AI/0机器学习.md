# 1、高等数学

## 1.1、导数

### 1.1.1、导数的概念
导数是微积分的核心概念之一，它深刻地描述了函数值变化的快慢程度。导数就是函数在某一点处的瞬时变化率。

### 1.1.2、基本函数的导数

![image.png|575](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250905143657.png)

### 1.1.3、导数的求导法则
![image.png|575](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250905144407.png)

![image.png|575](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250905150223.png)

### 1.1.4、利用导数求极值

导数为0的点称为函数的驻点，在这个类点上函数可能会取到极大值或极小值。进一步判断则需要知道导数在附近的符号。例如x的三次方在x=0处导数为0，但并不会取得极大值或极小值

### 1.1.5、二阶导数


表示一阶导数本身的变化率。也就是“变化率”的变化率。它描述了函数的斜率（即一阶导数）是增加还是减少，以及增加或减少得有多快
二阶导数 > 0，函数图像是向下凸的，形状像一个碗
二阶导数 < 0，函数图像是向上凸的，形状像一个拱顶

![image.png|600](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250905151958.png)

## 1.2、偏导与梯度

### 1.2.1、偏导数
想象一个函数的值取决于多个变量，比如 z = f(x, y)，它表示一个三维空间中的曲面。偏导数的核心思想是：只让一个变量变化，而将其他所有变量视为常数，然后研究函数关于这个单一变量的变化率。这就像在多元变化的世界中，我们一次只观察一个方向上的影响

### 1.2.2、方向导数

偏导数：只研究函数沿坐标轴方向（如x轴、y轴）的变化率。
方向导数：研究函数在任何一个给定方向上的变化率。比如东北方向45°、西南方向等。所以，方向导数的核心思想是：函数在任意一个指定方向上的瞬时变化率是多少

### 1.2.3、梯度
箭头的方向：指向最陡峭的上坡方向。即，沿着这个方向走，你的海拔上升得最快。

箭头的长度（模）：代表这个坡的陡峭程度。箭头越长，表示这个方向越陡；箭头越短，表示坡越平缓。如果箭头长度为零（||∇f|| = 0），说明你站在一个平地（可能是山顶、谷底或鞍点），所有方向都没有坡度。

所以，梯度的核心身份是：函数值增长最快的方向向导，并且其强度代表了增长的速率

# 2、线性代数

## 2.1、标量与向量
### 2.1.1、标量与向量的概念
标量是只有大小的量（如温度、距离），而向量是既有大小又有方向的量（如位移、力），两者的根本区别在于方向性

### 2.1.2、向量运算
1、向量转置：行转列，列转行
2、向量相加：对应位置相加
3、向量与标量相乘：标量与向量每个元素相乘
4、向量内积：又称向量点乘，两向量对应元素乘积之和，结果为标量

### 2.1.3、向量范数
范数是具有长度概念的函数
向量范数是衡量向量“长度”或“大小”的一种数学工具，它为一个向量赋予一个非负实数值

## 2.2、矩阵

### 2.2.1、矩阵基本概念
方阵：方阵是行数和列数**相等**的矩阵（即 `m = n`）。`n × n` 的方阵也称为 `n` 阶矩阵
对角矩阵：对角矩阵是一种方阵，其主对角线之外的所有元素都为零
单位矩阵：单位矩阵是一种特殊的对角矩阵，其主对角线上的所有元素都是 1，其余元素都是 0。它是矩阵乘法中的“单位元”，类似于数字 1 在乘法中的作用（任何数乘以 1 都不变）

### 2.2.2、矩阵乘法
两个矩阵可以相乘 当且仅当 第一个矩阵的列数等于第二个矩阵的行数
![image.png](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250906222642.png)

### 2.2.3、矩阵转置
矩阵的转置，直观来说就是将矩阵的行和列互相交换

### 2.2.4、矩阵的逆

对于一个 **n阶方阵** AA（即行数和列数相等的矩阵），如果存在另一个同阶方阵 BB，使得：

A×B=B×A=IA×B=B×A=I

其中 II 是 **n阶单位矩阵**（主对角线为1，其余为0的矩阵），那么矩阵 BB 就是 AA 的逆矩阵

![image.png](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250906231617.png)

### 2.2.5、其他矩阵运算

1、矩阵的向量化
矩阵的向量化 是一个将矩阵结构“展平”成一个长向量的操作，它在数值计算、机器学习和优化理论中极其重要。
**向量化** 是一个线性变换，它将一个 m×nm×n 的矩阵 AA 转换（展平）为一个 mn×1mn×1 的列向量。这个向量通常记作 vec(A)vec(A)
矩阵也可以进行行向量化rvec()

![image.png](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250906232323.png)

2、矩阵的内积
矩阵对应元素乘积之和，是一个标量

3、矩阵的Hadamard积
两个矩阵对应元素乘积

4、矩阵的Kronecker积
矩阵A每个元素与矩阵B的乘积，Kronecker积也称为直积或张量积

### 2.2.6、张量
张量可以认为是一个多维数组，是标量，1维向量和2维矩阵的n维推广

![image.png|575](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250907101335.png)

## 2.3、矩阵求导
### 2.3.1、典型计算场景
矩阵求导本质就是对函数变元的每个元素逐个求导，只是写成向量，矩阵的形式

### 2.3.3、梯度矩阵

# 3、概率论
## 3.1、概率
### 3.1.1、概率的概念
概率是对事件发生的可能性的度量。A发生的概率P(A)

### 3.1.2、概率的计算

![image.png|550](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250907111505.png)


## 3.2、概率分布

概率分布，是指用于表述随机变量取值的概率规律。事件的概率表示了一次试验中某一个结果发生的可能性大小。如果试验结果用变量x的取值来表示，则随机试验的概率分布就是随机变量的概率分布，即随机变量的可能取值及取得对应值的概率

### 3.2.1、均匀分布
均匀分布也叫矩形分布，它表示在相同长度间隔的分布概率是等可能的。均匀分布由两个参数a和b定义，它们是数轴上的最小值和最大值，通常缩写为U(a，b)。

### 3.2.2、正态分布
正态分布也称高斯分布，是常见的连续概率分布。正态分布在统计学上十分重要，经常用在自然和社会科学来代表一个不明的随机变量

## 3.3、贝叶斯定理

贝叶斯定理(Bayes'Theorem)是概率论中的一个核心定理，用于描述在已有条件概率信息的基础上，如何更新或计算事件的概率。它以英国数学家托马斯·贝叶斯的名字命名。贝叶斯定理特别适合处理“逆向概率”问题，即从结果反推原因的概率。

### 3.3.1、全概率公式
对于复杂事件 B，它可能有很多种具体情况，发生概率不容易直接求得
这些不同的具体情况可以是一组简单事件，记作 A1、A2、..、A”，发生的概率P(A)，如果它们满足两两互不相容、且发生根这样率之和为 1，就称它们是一个完备事件组。
如果知道了在每个简单事件发生的前提下、复杂事件发生的概率(条件概率P(BA))，就可以将它们全部合并起来，求出复杂事件的概率了。

### 3.3.2、贝叶斯公式
贝叶斯定理建立在条件概率的基础上，假设有两事件A,B，贝叶斯定理描述了在已知B发生的情况下，A发生的概率

## 3.4、似然函数
### 3.4.1、似然函数的概念


![image.png|525](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250907163224.png)

### 3.4.2、极大似然估计
![image.png|525](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250907163745.png)


# 1、机器学习概述
机器学习(Machine Learning,ML)主要研究计算机系统对于特定任务的性能，逐步进行改善的算法和统计模型。通过输入海量训练数据对模型进行训练，使模型掌握数据所蕴含的潜在规律，进而对新输入的数据进行准确的分类或预测。
机器学习是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸优化、算法复杂度理论等多门学科

## 1.1、人工智能、机器学习、深度学习

![image.png|500](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250907231923.png)


## 1.2、基本术语

l 数据集（Data Set）：多条记录的集合。
	Ø 训练集（Training Set）：用于训练模型的数据。
	Ø 验证集（Validation Set）：用于调节超参数的数据。
	Ø 测试集（Test Set）：用于评估模型性能的数据。
l 样本（Sample）：数据集中的一条记录是关于一个事件或对象的描述，称为一个样本。
l 特征（Feature）：数据集中一列反映事件或对象在某方面的表现或性质的事项，称为特征或属性。
l 特征向量（Feature Vector）：将样本的所有特征表示为向量的形式，输入到模型中。
l 标签（Label）：监督学习中每个样本的结果信息，也称作目标值（target）。
l 模型（Model）：一个机器学习算法与训练后的参数集合，用于进行预测或分类。
l 参数（Parameter）：模型通过训练学习到的值，例如线性回归中的权重和偏置。
l 超参数（Hyper Parameter）：由用户设置的参数，不能通过训练自动学习，例如学习率、正则化系数等。

# 2、机器学习基本理论

## 2.1、机器学习三要素
机器学习的方法一般主要由三部分构成：模型、策略和算法，可以认为：
**_机器学习方法 = 模型 + 策略 + 算法_**

l 模型（model）：总结数据的内在规律，用数学语言描述的参数系统
l 策略（strategy）：选取最优模型的评价准则
l 算法（algorithm）：选取最优模型的具体方法

## 2.2、机器学习方法分类
机器学习的方法种类繁多，并不存在一个统一的理论体系能够涵盖所有内容。从不同的角度，可以将机器学习的方法进行不同的分类：

通常分类：按照有无监督，机器学习可以分为 **有监督学习**、**无监督学习** 和 **半监督学习**，除此之外还有 **强化学习**。
按模型分类：根据模型性质，可以分为概率模型/非概率模型，线性/非线性模型等。
按学习技巧分类：根据算法基于的技巧，可以分为贝叶斯学习、核方法等

![image.png|625](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250907233142.png)

## 2.3、特征工程
特征工程（Feature Engineering）是机器学习过程中非常重要的一步，指的是通过对原始数据的处理、转换和构造，生成新的特征或选择有效的特征，从而提高模型的性能。简单来说，特征工程是将原始数据转换为可以更好地表示问题的特征形式，帮助模型更好地理解和学习数据中的规律。优秀的特征工程可以显著提高模型的表现；反之，忽视特征工程可能导致模型性能欠佳

### 2.3.1、特征工程内容
1）**特征选择**
从原始特征中挑选出与目标变量关系最密切的特征，剔除冗余、无关或噪声特征。这样可以减少模型的复杂度、加速训练过程、并减少过拟合的风险。
特征选择不会创建新特征，也不会改变数据结构。

（1）过滤法（Filter Method）
基于统计测试（如卡方检验、相关系数、信息增益等）来评估特征与目标变量之间的关系，选择最相关的特征。
（2）包裹法（Wrapper Method）
使用模型（如递归特征消除 RFE）来评估特征的重要性，并根据模型的表现进行特征选择。
（3）嵌入法（Embedded Method）
使用模型本身的特征选择机制（如决策树的特征重要性，L1正则化的特征选择）来选择最重要的特征。

2）**特征转换**

对数据进行数学或统计处理，使其变得更加适合模型的输入要求。
（1）归一化（Normalization）
将特征缩放到特定的范围（通常是0到1之间）。适用于对尺度敏感的模型（如KNN、SVM）。
（2）标准化（Standardization）
通过减去均值并除以标准差，使特征的分布具有均值0，标准差1。
（3）对数变换
对于有偏态的分布（如收入、价格等），对数变换可以将其转化为更接近正态分布的形式。
（4）类别变量的编码
独热编码（One-Hot Encoding）：将类别型变量转换为二进制列，常用于无序类别特征。
标签编码（Label Encoding）：将类别型变量映射为整数，常用于有序类别特征。
目标编码（Target Encoding）**：**将类别变量的每个类别替换为其对应目标变量的平均值或其他统计量。
频率编码（Frequency Encoding）**：**将类别变量的每个类别替换为该类别在数据集中的出现频率。

3）**特征构造**

特征构造是基于现有的特征创造出新的、更有代表性的特征。通过组合、转换、或者聚合现有的特征，形成能够更好反映数据规律的特征。
（1）交互特征
将两个特征组合起来，形成新的特征。例如，两个特征的乘积、和或差等。
例如，将年龄与收入结合创建新的特征，可能能更好地反映某些模式。
（2）统计特征
从原始特征中提取统计值，例如求某个时间窗口的平均值、最大值、最小值、标准差等。
例如，在时间序列数据中，你可以从原始数据中提取每个小时、每日的平均值。
（3）日期和时间特征
从日期时间数据中提取如星期几、月份、年份、季度等特征。
例如，将“2000-01-01”转换为“星期几”、“是否节假日”、“月初或月末”等特征。

4）**特征降维**
当数据集的特征数量非常大时，特征降维可以帮助减少计算复杂度并避免过拟合。通过降维方法，可以在保持数据本质的情况下减少特征的数量。

（1）主成分分析（PCA）
通过线性变换将原始特征映射到一个新的空间，使得新的特征（主成分）尽可能地保留数据的方差。
（2）线性判别分析（LDA）
一种监督学习的降维方法，通过最大化类间距离与类内距离的比率来降维。
（3）t-SNE（t-Distributed Stochastic Neighbor Embedding，t分布随机近邻嵌入）
一种非线性的降维技术，特别适合可视化高维数据。
（4）自编码器（Auto Encoder）
一种神经网络模型，通过压缩编码器来实现数据的降维。

### 2.3.2、常用方法
低方差过滤法
相关系数法
主成分分析（PCA）

## 2.4、模型评估和模型选择
### 2.4.1、损失函数
对于模型一次预测结果的好坏，需要有一个度量标准。
对于监督学习而言，给定一个输入_X_，选取的模型就相当于一个“决策函数”_f_，它可以输出一个预测结果_f(X)_，而真实的结果（标签）记为_Y_。_f(X)_ 和_Y_之间可能会有偏差，我们就用一个**损失函数**（loss function）来度量预测偏差的程度，记作 _L(Y,f(X))_。
l 损失函数用来衡量模型预测误差的大小；损失函数值越小，模型就越好；
l 损失函数是_f(X)_和_Y_的非负实值函数；
常见的损失函数有：

1）**0-1损失函数**
2）**平方损失函数**
3）**绝对损失函数**
4）**对数似然损失函数**

### 2.4.2、经验误差
根据选取的损失函数，就可以计算出模型_f(X)_在训练集上的平均误差，称为训练误差，也被称作 **经验误差**（empirical error） 或 **经验风险**（empirical risk）。 

类似地，在测试数据集上平均误差，被称为测试误差或者 **泛化误差**（generalization error）。

一般情况下对模型评估的策略，就是考察经验误差；当经验风险最小时，就认为取到了最优的模型。这种策略被称为 **经验风险最小化**（empirical risk minimization，ERM）。

### 2.4.3、欠拟合和过拟合
**拟合**（Fitting）是指机器学习模型在训练数据上学习到规律并生成预测结果的过程。理想情况下，模型能够准确地捕捉训练数据的模式，并且在未见过的新数据（测试数据）上也有良好的表现；即模型具有良好的 **泛化能力**。

![image.png|500](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250910144555.png)

**欠拟合**（Underfitting）：是指模型在训练数据上表现不佳，无法很好地捕捉数据中的规律。这样的模型不仅在训练集上表现不好，在测试集上也同样表现差。

![image.png|500](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250910144701.png)

过拟合（Overfitting）：是指模型在训练数据上表现得很好，但在测试数据或新数据上表现较差的情况。过拟合的模型对训练数据中的噪声或细节过度敏感，把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，从而失去了泛化能力。

![image.png|500](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250910144813.png)

（1）欠拟合

产生原因：

l 模型复杂度不足：模型过于简单，无法捕捉数据中的复杂关系。

l 特征不足：输入特征不充分，或者特征选择不恰当，导致模型无法充分学习数据的模式。

l 训练不充分：训练过程中迭代次数太少，模型没有足够的时间学习数据的规律。

l 过强的正则化：正则化项设置过大，强制模型过于简单，导致模型无法充分拟合数据。

解决办法：

Ø 增加模型复杂度：选择更复杂的模型。

Ø 增加特征或改进特征工程：添加更多的特征或通过特征工程来创造更有信息量的特征。

Ø 增加训练时间：增加训练的迭代次数，让模型有更多机会去学习。

Ø 减少正则化强度：如果使用了正则化，尝试减小正则化的权重，以让模型更灵活。

（2）过拟合

产生原因：

l 模型复杂度过高：模型过于复杂，参数太多。

l 训练数据不足：数据集太小，模型能记住训练数据的细节，但无法泛化到新数据。

l 特征过多：特征太多，模型可能会“记住”数据中的噪声，而不是学到真正的规律。

l 训练过长：训练时间过长，导致模型学习到训练数据中的噪声，而非数据的真正规律。

解决办法：

Ø 减少模型复杂度：降低模型的参数数量、使用简化的模型或降维来减小模型复杂度。

Ø 增加训练数据：收集更多数据，或通过数据增强来增加训练数据的多样性。

Ø 使用正则化：引入L1、L2正则化，避免过度拟合训练数据。

Ø 交叉验证：使用交叉验证技术评估模型在不同数据集上的表现，以减少过拟合的风险。

Ø 早停：训练时，当模型的验证损失不再下降时，提前停止训练，避免过度拟合训练集。

![image.png|500](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250910145732.png)

### 2.4.4、正则化
正则化（Regularization）是一种在训练机器学习模型时，在损失函数中添加额外项，来惩罚过大的参数，进而限制模型复杂度、避免过拟合，提高模型泛化能力的技术。
 **正则化系数**，用来表示惩罚项的权重。正则化系数不属于模型的参数，无法通过训练学习得到，需要在模型训练开始之前手动设置，这种参数被称为“**超参数**”。
两者相互平衡，在模型的拟合能力（偏差）和复杂度之间找到最佳折中。
常见的正则化技术有L1正则化和L2正则化。

### 2.4.5、交叉验证
交叉验证（Cross-Validation）是一种评估模型泛化能力的方法，通过将数据集划分为多个子集，反复进行训练和验证，以减少因单次数据划分带来的随机性误差。通过交叉验证能更可靠地估计模型在未知数据上的表现，亦能避免因单次数据划分不合理导致的模型过拟合或欠拟合。

1）**简单交叉验证（Hold-Out Validation）**

将数据划分为训练集和验证集（如70%训练，30%验证）。结果受单次划分影响较大，可能高估或低估模型性能。

2）**k折交叉验证（k-Fold Cross-Validation）**

将数据均匀分为k个子集（称为“折”），每次用k−1折训练，剩余1折验证，重复k次后取平均性能。充分利用数据，结果更稳定。

1）**留一交叉验证（Leave-One-Out，LOO）**
每次仅留一个样本作为验证集，其余全部用于训练，重复直到所有样本都被验证一次。适用于小数据集，计算成本极高。

## 2.5、模型求解算法
正则化可以有效防止过拟合，增强模型的泛化能力。这时模型的评估策略，就是让结构化的经验风险最小，即增加了正则化项的损失函数最小，称为 结构风险最小化（Structural Risk Minimization，SRM）这其实就是求解一个 **最优化问题**。
### 2.5.1、解析法
如果模型损失函数的最小值可以通过数学公式进行严格推导，得到一个解析解，那么就直接得到了最优模型的全部参数。这种方法称作解析法。

1）**特点**

l 适用条件：目标函数必须可导，且导数方程有解析解。
l 优点：直接且精确；计算高效
l 缺点：适用条件较为苛刻；特征维度较大时，矩阵求逆计算复杂度极高。

2）**应用示例**

l 线性回归问题：可以采用“最小二乘法”求得解析解。
l 线性回归L2正则化（Ridge回归，岭回归）

由于加入的对角矩阵就像一条“山岭”，因此L2正则化也称作“岭回归”。

### 2.5.2、梯度下降法

**梯度下降法**（gradient descent）是一种常用的一阶优化方法，是求解无约束优化问题最简单、最经典的方法之一。梯度下降法是迭代算法，基本思路就是先选取一个适当的初始值，然后沿着梯度方向或者负梯度方向，不停地更新参数，最终取到极小值

l 梯度方向：函数变化增长最快的方向（变量沿此方向变化时函数增长最快）
l 负梯度方向：函数变化减少最快的方向（变量沿此方向变化时函数减少最快）

因为损失函数是系数的函数，那么如果系数沿着损失函数的负梯度方向变化，此时损失函数减少最快，能够以最快速度下降到极小值。

## 2.6、模型评价指标

对学习的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价指标，也叫做性能度量
### 2.6.1、回归模型评价指标
模型的评价指标用于衡量模型在训练集或测试集上的性能，评估结果反映了模型预测的准确性和泛化能力。
对于回归问题，最常用的性能度量是“均方误差” (Mean Squared Error，MSE)。

1）**平均绝对误差（MAE）**
Ø MAE对异常值不敏感，解释直观。适用于数据包含异常值的场景。

2）**均方误差（MSE）**
Ø MSE会放大较大误差，对异常值敏感。适用于需要惩罚大误差的场景。

3）**均方根误差（RMSE）**
Ø 与MSE类似，但量纲与目标变量一致。适用于需要直观误差量纲的场景。如果一味地试图降低RMSE，可能会导致模型对异常值也拟合度很高，容易过拟合。

4）**R²（决定系数）**
Ø 衡量模型对目标变量的解释能力，越接近1越好，对异常值敏感。

### 2.6.2、分类模型评价指标
对于分类问题，最常用的指标就是“准确率”（Accuracy），它定义为分类器对测试集正确分类的样本数与总样本数之比。此外还有一系列常用的评价指标

1、混淆函数
混淆矩阵（Confusion Matrix）是用于评估分类模型性能的工具，展示了模型预测结果与实际标签的对比情况。

对于二分类问题，混淆矩阵是一个2×2矩阵：

![image.png|500](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250912092432.png)

例如，有10个样本。6个是猫，4个是狗。假设以猫为正例，模型预测对了5个猫，2个狗。

![image.png|500](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250912092507.png)

2、准确率（Accuracy）
正确预测的比例。

![image.png|500](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250912092614.png)

上述案例中，准确率 = (5+2) / 10 = 0.7

3、召回率
实际为正类的样本中预测为正类的比例，也叫查全率

![image.png](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250912092837.png)

![image.png|425](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250912092859.png)

上述案例中，召回率 = 5 / (5+1) = 0.8333

4、F1分数
精确率和召回率的调和平均
![image.png|425](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250912093637.png)

5、ROC曲线
l 真正例率（TPR）：实际为正例，被预测为正例的比例，即召回率。
l 假正例率（FPR）：实际为负例，被预测为正例的比例。
l 阈值（Threshold）：根据阈值将概率转换为类别标签。

ROC曲线（Receiver Operating Characteristic Curve，受试者工作特征）是评估二分类模型性能的工具，以假正例率（FPR）为横轴，以真正例率（TPR）为纵轴，展示不同阈值下模型的表现。绘制ROC曲线时，从高到低调整阈值，计算每个阈值的TPR和FPR并绘制所有阈值的点，形成ROC曲线。

6、AUC值
AUC值代表ROC曲线下的面积，用于量化模型性能。AUC值越大，模型区分正负类的能力越强，模型性能越好。AUC值=0.5表示模型接近随机猜测，AUC值=1代表完美模型。

# 3、KNN算法

## 3.1、介绍
K近邻算法（K-Nearest Neighbors，KNN）是一种基本的分类与回归方法，属于监督学习算法。其核心思想是通过计算给定样本与数据集中所有样本的距离，找到距离最近的K个样本，然后根据这K个样本的类别或值来预测当前样本的类别或值

![image.png|500](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250913120200.png)

### 3.1.1、工作原理
计算距离：计算待分类样本与训练集中每个样本的距离。
选择K个近邻：根据计算的距离，选择距离最近的K个样本。
投票或平均：
Ø 分类任务：统计K个近邻各类别的数量，将待分类样本归为数量最多的类别。
Ø 回归任务：取K个近邻的平均值作为预测结果。

### 3.1.2、关键参数
距离度量方法：选择合适的距离度量方法，常见的有欧氏距离、曼哈顿距离、切比雪夫距离、闵可夫斯基距离等。
K值：K值的选择对结果影响很大。K值过小容易过拟合，K值过大则可能欠拟合

### 3.1.3、优缺点
KNN优点：
l 简单直观，易于理解和实现。
l 无需训练过程，直接利用训练数据进行预测。

KNN缺点：
l 计算量大，尤其是训练集较大时。
l 对噪声数据较敏感。