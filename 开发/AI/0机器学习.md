# 1、高等数学

## 1.1、导数

### 1.1.1、导数的概念
导数是微积分的核心概念之一，它深刻地描述了函数值变化的快慢程度。导数就是函数在某一点处的瞬时变化率。

### 1.1.2、基本函数的导数

![image.png|575](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250905143657.png)

### 1.1.3、导数的求导法则
![image.png|575](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250905144407.png)

![image.png|575](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250905150223.png)

### 1.1.4、利用导数求极值

导数为0的点称为函数的驻点，在这个类点上函数可能会取到极大值或极小值。进一步判断则需要知道导数在附近的符号。例如x的三次方在x=0处导数为0，但并不会取得极大值或极小值

### 1.1.5、二阶导数


表示一阶导数本身的变化率。也就是“变化率”的变化率。它描述了函数的斜率（即一阶导数）是增加还是减少，以及增加或减少得有多快
二阶导数 > 0，函数图像是向下凸的，形状像一个碗
二阶导数 < 0，函数图像是向上凸的，形状像一个拱顶

![image.png|600](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250905151958.png)

## 1.2、偏导与梯度

### 1.2.1、偏导数
想象一个函数的值取决于多个变量，比如 z = f(x, y)，它表示一个三维空间中的曲面。偏导数的核心思想是：只让一个变量变化，而将其他所有变量视为常数，然后研究函数关于这个单一变量的变化率。这就像在多元变化的世界中，我们一次只观察一个方向上的影响

### 1.2.2、方向导数

偏导数：只研究函数沿坐标轴方向（如x轴、y轴）的变化率。
方向导数：研究函数在任何一个给定方向上的变化率。比如东北方向45°、西南方向等。所以，方向导数的核心思想是：函数在任意一个指定方向上的瞬时变化率是多少

### 1.2.3、梯度
箭头的方向：指向最陡峭的上坡方向。即，沿着这个方向走，你的海拔上升得最快。

箭头的长度（模）：代表这个坡的陡峭程度。箭头越长，表示这个方向越陡；箭头越短，表示坡越平缓。如果箭头长度为零（||∇f|| = 0），说明你站在一个平地（可能是山顶、谷底或鞍点），所有方向都没有坡度。

所以，梯度的核心身份是：函数值增长最快的方向向导，并且其强度代表了增长的速率

# 2、线性代数

## 2.1、标量与向量
### 2.1.1、标量与向量的概念
标量是只有大小的量（如温度、距离），而向量是既有大小又有方向的量（如位移、力），两者的根本区别在于方向性

### 2.1.2、向量运算
1、向量转置：行转列，列转行
2、向量相加：对应位置相加
3、向量与标量相乘：标量与向量每个元素相乘
4、向量内积：又称向量点乘，两向量对应元素乘积之和，结果为标量

### 2.1.3、向量范数
范数是具有长度概念的函数
向量范数是衡量向量“长度”或“大小”的一种数学工具，它为一个向量赋予一个非负实数值

## 2.2、矩阵

### 2.2.1、矩阵基本概念
方阵：方阵是行数和列数**相等**的矩阵（即 `m = n`）。`n × n` 的方阵也称为 `n` 阶矩阵
对角矩阵：对角矩阵是一种方阵，其主对角线之外的所有元素都为零
单位矩阵：单位矩阵是一种特殊的对角矩阵，其主对角线上的所有元素都是 1，其余元素都是 0。它是矩阵乘法中的“单位元”，类似于数字 1 在乘法中的作用（任何数乘以 1 都不变）

### 2.2.2、矩阵乘法
两个矩阵可以相乘 当且仅当 第一个矩阵的列数等于第二个矩阵的行数
![image.png](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250906222642.png)

### 2.2.3、矩阵转置
矩阵的转置，直观来说就是将矩阵的行和列互相交换

### 2.2.4、矩阵的逆

对于一个 **n阶方阵** AA（即行数和列数相等的矩阵），如果存在另一个同阶方阵 BB，使得：

A×B=B×A=IA×B=B×A=I

其中 II 是 **n阶单位矩阵**（主对角线为1，其余为0的矩阵），那么矩阵 BB 就是 AA 的逆矩阵

![image.png](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250906231617.png)

### 2.2.5、其他矩阵运算

1、矩阵的向量化
矩阵的向量化 是一个将矩阵结构“展平”成一个长向量的操作，它在数值计算、机器学习和优化理论中极其重要。
**向量化** 是一个线性变换，它将一个 m×nm×n 的矩阵 AA 转换（展平）为一个 mn×1mn×1 的列向量。这个向量通常记作 vec(A)vec(A)
矩阵也可以进行行向量化rvec()

![image.png](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250906232323.png)

2、矩阵的内积
矩阵对应元素乘积之和，是一个标量

3、矩阵的Hadamard积
两个矩阵对应元素乘积

4、矩阵的Kronecker积
矩阵A每个元素与矩阵B的乘积，Kronecker积也称为直积或张量积

### 2.2.6、张量
张量可以认为是一个多维数组，是标量，1维向量和2维矩阵的n维推广

![image.png|575](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250907101335.png)

## 2.3、矩阵求导
### 2.3.1、典型计算场景
矩阵求导本质就是对函数变元的每个元素逐个求导，只是写成向量，矩阵的形式

### 2.3.3、梯度矩阵

# 3、概率论
## 3.1、概率
### 3.1.1、概率的概念
概率是对事件发生的可能性的度量。A发生的概率P(A)

### 3.1.2、概率的计算

![image.png|550](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250907111505.png)


## 3.2、概率分布

概率分布，是指用于表述随机变量取值的概率规律。事件的概率表示了一次试验中某一个结果发生的可能性大小。如果试验结果用变量x的取值来表示，则随机试验的概率分布就是随机变量的概率分布，即随机变量的可能取值及取得对应值的概率

### 3.2.1、均匀分布
均匀分布也叫矩形分布，它表示在相同长度间隔的分布概率是等可能的。均匀分布由两个参数a和b定义，它们是数轴上的最小值和最大值，通常缩写为U(a，b)。

### 3.2.2、正态分布
正态分布也称高斯分布，是常见的连续概率分布。正态分布在统计学上十分重要，经常用在自然和社会科学来代表一个不明的随机变量

## 3.3、贝叶斯定理

贝叶斯定理(Bayes'Theorem)是概率论中的一个核心定理，用于描述在已有条件概率信息的基础上，如何更新或计算事件的概率。它以英国数学家托马斯·贝叶斯的名字命名。贝叶斯定理特别适合处理“逆向概率”问题，即从结果反推原因的概率。

### 3.3.1、全概率公式
对于复杂事件 B，它可能有很多种具体情况，发生概率不容易直接求得
这些不同的具体情况可以是一组简单事件，记作 A1、A2、..、A”，发生的概率P(A)，如果它们满足两两互不相容、且发生根这样率之和为 1，就称它们是一个完备事件组。
如果知道了在每个简单事件发生的前提下、复杂事件发生的概率(条件概率P(BA))，就可以将它们全部合并起来，求出复杂事件的概率了。

### 3.3.2、贝叶斯公式
贝叶斯定理建立在条件概率的基础上，假设有两事件A,B，贝叶斯定理描述了在已知B发生的情况下，A发生的概率

## 3.4、似然函数
### 3.4.1、似然函数的概念


![image.png|525](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250907163224.png)

### 3.4.2、极大似然估计
![image.png|525](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250907163745.png)


# 1、机器学习概述
机器学习(Machine Learning,ML)主要研究计算机系统对于特定任务的性能，逐步进行改善的算法和统计模型。通过输入海量训练数据对模型进行训练，使模型掌握数据所蕴含的潜在规律，进而对新输入的数据进行准确的分类或预测。
机器学习是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸优化、算法复杂度理论等多门学科

## 1.1、人工智能、机器学习、深度学习

![image.png|500](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250907231923.png)


## 1.2、基本术语

l 数据集（Data Set）：多条记录的集合。
	Ø 训练集（Training Set）：用于训练模型的数据。
	Ø 验证集（Validation Set）：用于调节超参数的数据。
	Ø 测试集（Test Set）：用于评估模型性能的数据。
l 样本（Sample）：数据集中的一条记录是关于一个事件或对象的描述，称为一个样本。
l 特征（Feature）：数据集中一列反映事件或对象在某方面的表现或性质的事项，称为特征或属性。
l 特征向量（Feature Vector）：将样本的所有特征表示为向量的形式，输入到模型中。
l 标签（Label）：监督学习中每个样本的结果信息，也称作目标值（target）。
l 模型（Model）：一个机器学习算法与训练后的参数集合，用于进行预测或分类。
l 参数（Parameter）：模型通过训练学习到的值，例如线性回归中的权重和偏置。
l 超参数（Hyper Parameter）：由用户设置的参数，不能通过训练自动学习，例如学习率、正则化系数等。

# 2、机器学习基本理论

## 2.1、机器学习三要素
机器学习的方法一般主要由三部分构成：模型、策略和算法，可以认为：
**_机器学习方法 = 模型 + 策略 + 算法_**

l 模型（model）：总结数据的内在规律，用数学语言描述的参数系统
l 策略（strategy）：选取最优模型的评价准则
l 算法（algorithm）：选取最优模型的具体方法

## 2.2、机器学习方法分类
机器学习的方法种类繁多，并不存在一个统一的理论体系能够涵盖所有内容。从不同的角度，可以将机器学习的方法进行不同的分类：

通常分类：按照有无监督，机器学习可以分为 **有监督学习**、**无监督学习** 和 **半监督学习**，除此之外还有 **强化学习**。
按模型分类：根据模型性质，可以分为概率模型/非概率模型，线性/非线性模型等。
按学习技巧分类：根据算法基于的技巧，可以分为贝叶斯学习、核方法等

![image.png|625](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250907233142.png)

## 2.3、特征工程
特征工程（Feature Engineering）是机器学习过程中非常重要的一步，指的是通过对原始数据的处理、转换和构造，生成新的特征或选择有效的特征，从而提高模型的性能。简单来说，特征工程是将原始数据转换为可以更好地表示问题的特征形式，帮助模型更好地理解和学习数据中的规律。优秀的特征工程可以显著提高模型的表现；反之，忽视特征工程可能导致模型性能欠佳

### 2.3.1、特征工程内容
1）**特征选择**
从原始特征中挑选出与目标变量关系最密切的特征，剔除冗余、无关或噪声特征。这样可以减少模型的复杂度、加速训练过程、并减少过拟合的风险。
特征选择不会创建新特征，也不会改变数据结构。

（1）过滤法（Filter Method）
基于统计测试（如卡方检验、相关系数、信息增益等）来评估特征与目标变量之间的关系，选择最相关的特征。
（2）包裹法（Wrapper Method）
使用模型（如递归特征消除 RFE）来评估特征的重要性，并根据模型的表现进行特征选择。
（3）嵌入法（Embedded Method）
使用模型本身的特征选择机制（如决策树的特征重要性，L1正则化的特征选择）来选择最重要的特征。

2）**特征转换**

对数据进行数学或统计处理，使其变得更加适合模型的输入要求。
（1）归一化（Normalization）
将特征缩放到特定的范围（通常是0到1之间）。适用于对尺度敏感的模型（如KNN、SVM）。
（2）标准化（Standardization）
通过减去均值并除以标准差，使特征的分布具有均值0，标准差1。
（3）对数变换
对于有偏态的分布（如收入、价格等），对数变换可以将其转化为更接近正态分布的形式。
（4）类别变量的编码
独热编码（One-Hot Encoding）：将类别型变量转换为二进制列，常用于无序类别特征。
标签编码（Label Encoding）：将类别型变量映射为整数，常用于有序类别特征。
目标编码（Target Encoding）**：**将类别变量的每个类别替换为其对应目标变量的平均值或其他统计量。
频率编码（Frequency Encoding）**：**将类别变量的每个类别替换为该类别在数据集中的出现频率。

3）**特征构造**

特征构造是基于现有的特征创造出新的、更有代表性的特征。通过组合、转换、或者聚合现有的特征，形成能够更好反映数据规律的特征。
（1）交互特征
将两个特征组合起来，形成新的特征。例如，两个特征的乘积、和或差等。
例如，将年龄与收入结合创建新的特征，可能能更好地反映某些模式。
（2）统计特征
从原始特征中提取统计值，例如求某个时间窗口的平均值、最大值、最小值、标准差等。
例如，在时间序列数据中，你可以从原始数据中提取每个小时、每日的平均值。
（3）日期和时间特征
从日期时间数据中提取如星期几、月份、年份、季度等特征。
例如，将“2000-01-01”转换为“星期几”、“是否节假日”、“月初或月末”等特征。

4）**特征降维**
当数据集的特征数量非常大时，特征降维可以帮助减少计算复杂度并避免过拟合。通过降维方法，可以在保持数据本质的情况下减少特征的数量。

（1）主成分分析（PCA）
通过线性变换将原始特征映射到一个新的空间，使得新的特征（主成分）尽可能地保留数据的方差。
（2）线性判别分析（LDA）
一种监督学习的降维方法，通过最大化类间距离与类内距离的比率来降维。
（3）t-SNE（t-Distributed Stochastic Neighbor Embedding，t分布随机近邻嵌入）
一种非线性的降维技术，特别适合可视化高维数据。
（4）自编码器（Auto Encoder）
一种神经网络模型，通过压缩编码器来实现数据的降维。

### 2.3.2、常用方法
低方差过滤法
相关系数法
主成分分析（PCA）

## 2.4、模型评估和模型选择
### 2.4.1、损失函数
对于模型一次预测结果的好坏，需要有一个度量标准。
对于监督学习而言，给定一个输入_X_，选取的模型就相当于一个“决策函数”_f_，它可以输出一个预测结果_f(X)_，而真实的结果（标签）记为_Y_。_f(X)_ 和_Y_之间可能会有偏差，我们就用一个**损失函数**（loss function）来度量预测偏差的程度，记作 _L(Y,f(X))_。
l 损失函数用来衡量模型预测误差的大小；损失函数值越小，模型就越好；
l 损失函数是_f(X)_和_Y_的非负实值函数；
常见的损失函数有：

1）**0-1损失函数**
2）**平方损失函数**
3）**绝对损失函数**
4）**对数似然损失函数**

### 2.4.2、经验误差
根据选取的损失函数，就可以计算出模型_f(X)_在训练集上的平均误差，称为训练误差，也被称作 **经验误差**（empirical error） 或 **经验风险**（empirical risk）。 

类似地，在测试数据集上平均误差，被称为测试误差或者 **泛化误差**（generalization error）。

一般情况下对模型评估的策略，就是考察经验误差；当经验风险最小时，就认为取到了最优的模型。这种策略被称为 **经验风险最小化**（empirical risk minimization，ERM）。

### 2.4.3、欠拟合和过拟合
**拟合**（Fitting）是指机器学习模型在训练数据上学习到规律并生成预测结果的过程。理想情况下，模型能够准确地捕捉训练数据的模式，并且在未见过的新数据（测试数据）上也有良好的表现；即模型具有良好的 **泛化能力**。

![image.png|500](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250910144555.png)

**欠拟合**（Underfitting）：是指模型在训练数据上表现不佳，无法很好地捕捉数据中的规律。这样的模型不仅在训练集上表现不好，在测试集上也同样表现差。

![image.png|500](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250910144701.png)

过拟合（Overfitting）：是指模型在训练数据上表现得很好，但在测试数据或新数据上表现较差的情况。过拟合的模型对训练数据中的噪声或细节过度敏感，把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，从而失去了泛化能力。

![image.png|500](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250910144813.png)

（1）欠拟合

产生原因：

l 模型复杂度不足：模型过于简单，无法捕捉数据中的复杂关系。

l 特征不足：输入特征不充分，或者特征选择不恰当，导致模型无法充分学习数据的模式。

l 训练不充分：训练过程中迭代次数太少，模型没有足够的时间学习数据的规律。

l 过强的正则化：正则化项设置过大，强制模型过于简单，导致模型无法充分拟合数据。

解决办法：

Ø 增加模型复杂度：选择更复杂的模型。

Ø 增加特征或改进特征工程：添加更多的特征或通过特征工程来创造更有信息量的特征。

Ø 增加训练时间：增加训练的迭代次数，让模型有更多机会去学习。

Ø 减少正则化强度：如果使用了正则化，尝试减小正则化的权重，以让模型更灵活。

（2）过拟合

产生原因：

l 模型复杂度过高：模型过于复杂，参数太多。

l 训练数据不足：数据集太小，模型能记住训练数据的细节，但无法泛化到新数据。

l 特征过多：特征太多，模型可能会“记住”数据中的噪声，而不是学到真正的规律。

l 训练过长：训练时间过长，导致模型学习到训练数据中的噪声，而非数据的真正规律。

解决办法：

Ø 减少模型复杂度：降低模型的参数数量、使用简化的模型或降维来减小模型复杂度。

Ø 增加训练数据：收集更多数据，或通过数据增强来增加训练数据的多样性。

Ø 使用正则化：引入L1、L2正则化，避免过度拟合训练数据。

Ø 交叉验证：使用交叉验证技术评估模型在不同数据集上的表现，以减少过拟合的风险。

Ø 早停：训练时，当模型的验证损失不再下降时，提前停止训练，避免过度拟合训练集。

![image.png|500](https://yancey-note-img.oss-cn-beijing.aliyuncs.com/20250910145732.png)

### 2.4.4、正则化
